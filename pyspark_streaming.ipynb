{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZLSHrjU+1oiKXyk5PULas",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfcastillo90/pyspark_streaming_ubo/blob/main/pyspark_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4GHGEVt-BUj"
      },
      "outputs": [],
      "source": [
        "## descargamos e instalamos jdk y apache spark\n",
        "%%capture\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget 'https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz' -O 'spark-3.3.1-bin-hadoop3.tgz'\n",
        "!tar -xvf /content/spark-3.3.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark \n",
        "import os \n",
        "import findspark \n",
        "## creamos las variables de entorno JAVA_HOME y SPARK_HOME\n",
        "os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME']='/content/spark-3.3.1-bin-hadoop3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inicializamos\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "2aJCnf-V-aj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## nos integramos con google drive\n",
        "from google.colab import drive \n",
        "drive.mount(\"/content/gdrive\")\n",
        "gdrive='content/gdrive/MyDrive/Estudios/UBO/BIGDATA'"
      ],
      "metadata": {
        "id": "BV8jMOL1-d-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## creamos la carpeta streaming, primero validamos si existe o no\n",
        "!test -d /content/streaming && echo 'la carpeta existe' || mkdir streaming"
      ],
      "metadata": {
        "id": "MzTSGqVH-iBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.master('local').appName('PySparkStreaming').config('spark.ui.port',4050).getOrCreate()"
      ],
      "metadata": {
        "id": "_ft6bMuF-m0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Definimos el esquema\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "structureSchema = StructType([ \\\n",
        "                     StructField('index', IntegerType(), True), \\\n",
        "                     StructField('Order ID', StringType(), True), \\\n",
        "                     StructField('Date', StringType(), True), \\\n",
        "                     StructField('Status', StringType(), True), \\\n",
        "                     ])\n"
      ],
      "metadata": {
        "id": "v7O0E3vY-p_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Leemos el archivo csv y le asignamos el esquema que fue definido previamente\n",
        "dataFrame = spark.read.csv('/'+gdrive+'/amazon-orders-status.csv', sep=',', header=True, schema=structureSchema)\n"
      ],
      "metadata": {
        "id": "mSlAYfy5-sZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.printSchema()"
      ],
      "metadata": {
        "id": "1-vVDq61-56G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDataFrame = spark.readStream.format('csv').schema(structureSchema).option('header', True).load('streaming')"
      ],
      "metadata": {
        "id": "DjFZAc0H-9cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDataFrame.isStreaming"
      ],
      "metadata": {
        "id": "WjOWAaaf_ACH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H1CwiDut_DjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5WlLOgg-9ut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}