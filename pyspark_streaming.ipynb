{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrR6WxaJHumvwN+iDc7/d5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alfcastillo90/pyspark_streaming_ubo/blob/main/pyspark_streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "W4GHGEVt-BUj"
      },
      "outputs": [],
      "source": [
        "## descargamos e instalamos jdk y apache spark\n",
        "%%capture\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget 'https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz' -O 'spark-3.3.1-bin-hadoop3.tgz'\n",
        "!tar -xvf /content/spark-3.3.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark \n",
        "!pip install background\n",
        "import os \n",
        "import findspark \n",
        "## creamos las variables de entorno JAVA_HOME y SPARK_HOME\n",
        "os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME']='/content/spark-3.3.1-bin-hadoop3'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## inicializamos\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "2aJCnf-V-aj0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## nos integramos con google drive\n",
        "from google.colab import drive \n",
        "drive.mount(\"/content/gdrive\")\n",
        "gdrive='content/gdrive/MyDrive/Estudios/UBO/BIGDATA'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV8jMOL1-d-3",
        "outputId": "1dcbb2ca-1025-4f3b-dda6-930b2488e0bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## creamos la carpeta streaming, primero validamos si existe o no\n",
        "!test -d /content/streaming && echo 'la carpeta existe' || mkdir streaming"
      ],
      "metadata": {
        "id": "MzTSGqVH-iBJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession.builder.master('local').appName('PySparkStreaming').config('spark.ui.port',4050).getOrCreate()"
      ],
      "metadata": {
        "id": "_ft6bMuF-m0Q"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Definimos el esquema\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "structureSchema = StructType([ \\\n",
        "                     StructField('index', IntegerType(), True), \\\n",
        "                     StructField('Order ID', StringType(), True), \\\n",
        "                     StructField('Date', StringType(), True), \\\n",
        "                     StructField('Status', StringType(), True), \\\n",
        "                     ])\n"
      ],
      "metadata": {
        "id": "v7O0E3vY-p_q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Leemos el archivo csv y le asignamos el esquema que fue definido previamente\n",
        "dataFrame = spark.read.csv('/'+gdrive+'/amazon-orders-status.csv', sep=',', header=True, schema=structureSchema)\n"
      ],
      "metadata": {
        "id": "mSlAYfy5-sZd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-vVDq61-56G",
        "outputId": "dc94b157-475f-40d5-e37f-9e6f6274d5fa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- index: integer (nullable = true)\n",
            " |-- Order ID: string (nullable = true)\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Status: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDataFrame = spark.readStream.format('csv').schema(structureSchema).option('header', True).load('streaming')"
      ],
      "metadata": {
        "id": "DjFZAc0H-9cW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streamingDataFrame.isStreaming"
      ],
      "metadata": {
        "id": "WjOWAaaf_ACH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgngjViuMjtF",
        "outputId": "9746f203-3d75-440f-d1a7-dc0de434f43b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128975"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from background import task\n",
        "from time import sleep\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "\n",
        "@task\n",
        "def index(n):\n",
        "  pd.DataFrame(dataFrame.filter((dataFrame['index'] >= n) & (dataFrame['index'] <= (n +10 ))).toPandas()).to_csv('/content/streaming/data'+ str(n)+'.csv' ,index=False)\n",
        "\n",
        "def ciclo():\n",
        "  for n in range(0,dataFrame.count()):\n",
        "    index(n)\n",
        "\n",
        "ciclo()"
      ],
      "metadata": {
        "id": "H1CwiDut_DjO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df=streamingDataFrame.select('*')\n",
        "query=(result_df.writeStream.format('json').queryName('selectTable')\\\n",
        "       .option('checkpointLocation','checkpoint')\\\n",
        "       .option('path','results')\\\n",
        "       .start()\\\n",
        "       .awaitTermination())"
      ],
      "metadata": {
        "id": "z1SQnFLp_oLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5WlLOgg-9ut"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}